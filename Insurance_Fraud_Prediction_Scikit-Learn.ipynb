{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Myth: Fraud is a victimless crime.\n",
    "\n",
    "## Reality : Insurance fraud is one of the biggest crimes in the U.S.\n",
    "\n",
    "- Every year claims and underwriting fraud cost **$80 billion**\n",
    "- Fraudulent claims account for **5-10%** of all claims \n",
    "\n",
    "![Image](https://www.wnsdecisionpoint.com/Portals/1/Images/Reports-Images/Insurance-Fraud-Detection/exhibit-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Impact of Fraud\n",
    "\n",
    "- Fraudulent claims directly impact Loss Ratio, thereby, reducing profitability and also negatively impacting Return on Equity (ROE)\n",
    "- Fraudulent claims add to premium costs, since insurers are compelled to pass on the cost of such claims to policyholders\n",
    "\n",
    "## Business Need\n",
    "\n",
    "- Control dishonest claims payout through better fraud detection techniques\n",
    "- Automation of Fraud Detection Approach making better use of data\n",
    "\n",
    "![Image](https://www.wnsdecisionpoint.com/Portals/1/Images/Reports-Images/Insurance-Fraud-Detection/exhibit-4.jpg)\n",
    "\n",
    "\n",
    "![Image](https://www.wnsdecisionpoint.com/Portals/1/Images/Reports-Images/Insurance-Fraud-Detection/exhibit-5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Detecting insurance fraud poses an interesting problem from a data science perspective as the problem becomes a binary classification problem in which our “response” variable is “Y” or “N” — claim is fraudulent or not.\n",
    "\n",
    "![Image](https://www.wnsdecisionpoint.com/Portals/1/Images/Reports-Images/Insurance-Fraud-Detection/exhibit-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Image](https://www.wnsdecisionpoint.com/Portals/1/Images/Reports-Images/Insurance-Fraud-Detection/exhibit-12-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll demonstrate an attempt to solve this problem by using an auto insurance dataset containing 1000 observations on 39 features, and using this data to train a classifier to be able to predict whether a particular claim is fraudulent or not.\n",
    "\n",
    "The [dataset](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4954928053318020/1058911316420443/167703932442645/latest.html) is made available by APN Partner - [Databricks](https://databricks.com/), a company that provides an unified analytics platform, nuifying data science and engineering across the Machine Learning lifecycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"insurance_claims.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data exploration\n",
    "\n",
    "Display number of observations (rows) and features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quick look at some sample observations reveal the dataset is not the cleanest:\n",
    "- Some features have wide variations in values\n",
    "- Some features have missing values\n",
    "- There is a mix of numberic and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stringCols = []\n",
    "for i,t in enumerate(data.dtypes):\n",
    "    if t == 'object':\n",
    "        stringCols.append(data.columns[i])\n",
    "        \n",
    "for col in stringCols:        \n",
    "    print(\"{} : {}\".format(col, len(data.groupby(col).policy_number.nunique())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Based on a count of categorical variables, it can be seen that following fields have too many categories, hence cannot be encoded into numeric values meaningfully.\n",
    "- `policy_bind_date`\n",
    "- `incident_date`\n",
    "- `incident_location`\n",
    "\n",
    "Among numeric columns, `insured_zip` is in reality a categorical value, and cannot be encoded meaningfully.\n",
    "\n",
    "In addition, `policy_number` serves as an idnetifier, and likely have no effect on a claim being fraudulent or not.\n",
    "\n",
    "These columns are therefore removed, for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "colsToDelete = [\"policy_number\", \"policy_bind_date\", \"insured_zip\", \"incident_location\", \"incident_date\"]\n",
    "\n",
    "for col in colsToDelete:\n",
    "    del data[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remaining columns will be encoded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "filteredStringColList = [i for i in stringCols if i not in colsToDelete]\n",
    "print(filteredStringColList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The target variable, to be predicted is `fraud-reported`. Plotting the count of observations belonging the two classes `Y` and `N` also reveals a large imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data['fraud_reported'].value_counts().plot(kind='bar', color=plt.cm.Set1(np.arange(len(data))), rot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Analyzing the location of claims using the feature `incident_state` reveals this dataset only contains records from mid-Atlantic states. \n",
    "\n",
    "Plotting the claims data on a map provides a nice visual on which area are more rpone to insurance claims, which could prove to be a valuable insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data['incident_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data['incident_state_count'] = data['incident_state']\n",
    "numCol = data.shape[1] -1\n",
    "for i in range(len(data['incident_state_count'])):\n",
    "    if data.iloc[i, numCol] == \"NY\":\n",
    "        data.iloc[i, numCol] = 262\n",
    "    if data.iloc[i, numCol] == \"SC\":\n",
    "        data.iloc[i, numCol] = 248\n",
    "    if data.iloc[i, numCol] == \"WV\":\n",
    "        data.iloc[i, numCol] = 217\n",
    "    if data.iloc[i, numCol] == \"VA\":\n",
    "        data.iloc[i, numCol] = 110\n",
    "    if data.iloc[i, numCol] == \"NC\":\n",
    "        data.iloc[i, numCol] = 110\n",
    "    if data.iloc[i, numCol] == \"PA\":\n",
    "        data.iloc[i, numCol] = 30\n",
    "    if data.iloc[i, numCol] == \"OH\":\n",
    "        data.iloc[i, numCol] = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chart_studio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "import chart_studio.plotly as cplt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "cplt.sign_in(username='binoyd', api_key='AqufAmuAZYO1iXQXPVnU')\n",
    "\n",
    "plotdata = [go.Choropleth(autocolorscale = True, locations = data['incident_state'],\n",
    "                      z = data['incident_state_count'],\n",
    "                      locationmode = 'USA-states',\n",
    "                      marker = go.choropleth.Marker(line = go.choropleth.marker.Line(color = 'rgb(255,255,255)', width = 2)),\n",
    "                      colorbar = go.choropleth.ColorBar(title = \"Number of Incidents\"))]\n",
    "layout = go.Layout(\n",
    "    title = go.layout.Title(\n",
    "        text = 'Insurance Incident Claims by State'\n",
    "    ),\n",
    "    geo = go.layout.Geo(\n",
    "        scope = 'usa',\n",
    "        projection = go.layout.geo.Projection(type = 'albers usa'),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(255, 255, 255)'),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = plotdata, layout = layout)\n",
    "cplt.iplot(fig, filename = 'd3-cloropleth-map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['incident_state_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plotting total claim amount report by year shows no pattern in claims, meaning date of claim of likely have not much influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(data, x='auto_year', y='total_claim_amount')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the data is ready to split into `X` and `Y`, representing features and target, with categorical variables encoded into numeric ones.\n",
    "\n",
    "Note, that number of columns go up, since each categorical column now gets expanded into a column for each of it's distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns[0]:data.columns[data.shape[1]-2]]\n",
    "X = pd.get_dummies(X)\n",
    "Y = data[data.columns[data.shape[1]-1]]\n",
    "Y = Y.replace(\"Y\", 1)\n",
    "Y = Y.replace(\"N\", 0)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LogisticRegression - Random Cross-validation\n",
    "\n",
    "The problem we have at hand now is a binary classification, where we have to predict observations into one of two classes - `1` or `0` (corresponding to whether a particular claim is identified as frauduent or not)\n",
    "\n",
    "This can be achieved using different algorithms, such as LogisticRegression, RandomForest, DecisionTree, SVM, etc., with various hyper-paramaeters, such as penalty function used, number of estimators, maximum depth of tree etc.\n",
    "\n",
    "Before doing a full Grid search, our first attempt is to run a Logistic Regression model through grid search and random cross validation, using Scikit-learn libraries, that are computationally inexpensive.\n",
    "\n",
    "Grid search is performed using a LogisticRegresion learner with `lblinear` solver, with hyperparameter space defined over penalty (L1 and L2 norms) and inverse regularization strenth ( 0  to 10, with $1/10^6$ fold increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def perform_randomized_search(features, target, model, hyperparams, kFolds):\n",
    "    randomizedsearch = RandomizedSearchCV(model, hyperparams, cv = kFolds, verbose=1)\n",
    "    best_model = randomizedsearch.fit(features, target)\n",
    "    print(\"The mean accuracy of the model is:\",best_model.score(features, target))\n",
    "    print(\"The best parameters for the model are:\",best_model.best_params_)\n",
    "    return best_model\n",
    "    \n",
    "logistic = linear_model.LogisticRegression(class_weight = 'balanced', solver='liblinear', max_iter=100)\n",
    "norms = ['l1', 'l2']\n",
    "C = np.random.uniform(0, 10, 100000)\n",
    "hyperparameters = dict(C=C, penalty=norms)\n",
    "model = perform_randomized_search(X, Y, logistic, hyperparameters, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification report - Logistic Regression\n",
    "As a result of this grid search, we obtain a model that yields an accuracy of close to 88%.\n",
    "\n",
    "However accuracy in itself is not of much value, specially when dealing with classification with imbalanced classes. In this particular case, as we saw earlier, there is a large imbalance where much more claims are identified to be not fraudulent in training data than the ones that are reported to be fraudulent.\n",
    "\n",
    "In reality, insurance industry would prefer to predict as many true negatives (under represented class) to minimize the loss.\n",
    "\n",
    "So we create a classification report to see how our Logistic Regression model performed in predicting both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=30)\n",
    "\n",
    "logReg = linear_model.LogisticRegression(C=2.2014271313563083, penalty='l1', solver='liblinear')\n",
    "logReg.fit(X_train, y_train)\n",
    "y_pred = logReg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In spite of relative difficulty in predicting negative samples, the Logistic regression still predicts fraudult claims correctly for about 62% of time.\n",
    "\n",
    "Even though the preliminary test looks good, and feels like we can proceed to productionize this model with Logistic regression, it would be prudent to do a search on other algortihms avaiable for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GridSearch with Pipeline\n",
    "\n",
    "Instead of running different classifiers separately and comparining the outcomes manually, in Scikit learn we can use Pipeline feature with a list of classifiers. \n",
    "\n",
    "We then conduct a Grid search over the algorithms and hyperparameters specified for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "## We will include our logistic regression models in addition to RandomForestClassifier and DecisionTreeClassifier\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"classifier\": [LogisticRegression()], \n",
    "        \"classifier__penalty\": ['l2','l1'], \n",
    "        \"classifier__C\": np.logspace(0, 10, 50)\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [LogisticRegression()], \n",
    "        \"classifier__penalty\": ['l2'], \n",
    "        \"classifier__C\": np.logspace(0, 10, 50),\n",
    "        \"classifier__solver\":['newton-cg','saga','sag','liblinear']\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [RandomForestClassifier()],\n",
    "        \"classifier__n_estimators\": [10, 100, 1000, 1000],\n",
    "        \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "        \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "        \"classifier__max_leaf_nodes\": [2, 5,10]\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [DecisionTreeClassifier()],\n",
    "        \"classifier__splitter\":['best', 'random'],\n",
    "        \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "        \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "        \"classifier__max_leaf_nodes\": [2, 5,10]\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [DecisionTreeClassifier(class_weight = 'balanced')],\n",
    "        \"classifier__splitter\":['best', 'random'],\n",
    "        \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "        \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "        \"classifier__max_leaf_nodes\": [2, 5,10]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def execute_pipeline(features,target, model_list, kFolds):\n",
    "    pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
    "    gridsearch = GridSearchCV(pipe, model_list, cv=kFolds, verbose=1, n_jobs=-1) # Fit grid search\n",
    "    best_model = gridsearch.fit(features, target)\n",
    "    print(\"The mean accuracy of the model is:\",best_model.score(features, target))\n",
    "    print(\"The best parameters for the model are:\",best_model.best_params_)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "model = execute_pipeline(X, Y, models, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GridSearch finds Decision Tree Classifier to be the best performing model, although the accuracy of 86% is clearly lower than that of Logistic Regression. \n",
    "\n",
    "But then again, only accuracy measure is not that relevant given teh class imbalance that we noticed for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification report - DecisionTreeClassifier\n",
    "\n",
    "\n",
    "We therefore run a classification report using the identified classifier from GridSearch, so as to be able to compare it with the one obtained for Logistic Regression earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "decT = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
    "            max_features=None, max_leaf_nodes=5, min_impurity_decrease=0.0,\n",
    "            min_impurity_split=None, min_samples_leaf=10,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='random')\n",
    "\n",
    "decT.fit(X_train, y_train)\n",
    "y_pred = decT.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing the classification report, we discover that the Decision Tree classifier yields a much higher precision of 94% for sample class - `0` (NOT Fraudulent).\n",
    "\n",
    "However, the precision for sample class - `1` (Fraudulent) is lower at 59% (as opposed to 63% as obtained from Logistic regression).\n",
    "\n",
    "Since, as discussed before, correctly identifying fraudulent claims is more important for insurance claims, it would be safe to proceed with Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "With the insight derived thus far, we'll now proceed to create a SageMaker training and deployment pipeleine using one of the SageMaker native algorithm - Linear Learner Algorithm.\n",
    "\n",
    "Linear Learner Algorithm can be used for either regression or classification. When used for Classification, it is essentially same as using a LogisticRegression classifier with `liblinear` solver."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
